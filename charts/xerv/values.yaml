# XERV Helm Chart Values
#
# For full documentation, see: https://github.com/ml-rust/xerv

# Number of replicas
# For Raft backend, use odd numbers (1, 3, 5) for quorum
replicaCount: 1

image:
  repository: ghcr.io/ml-rust/xerv
  tag: ""  # Defaults to Chart.appVersion
  pullPolicy: IfNotPresent

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

# Dispatch backend configuration
dispatch:
  # Backend type: memory, raft, redis, nats
  backend: memory

  # Redis configuration (when backend is "redis")
  redis:
    # External Redis URL (required for redis backend)
    url: ""
    # Use Redis Streams for reliable delivery
    useStreams: true
    # Connection pool size
    poolSize: 10
    # Consumer group name
    consumerGroup: xerv-workers

  # NATS configuration (when backend is "nats")
  nats:
    # External NATS URL (required for nats backend)
    url: ""
    # Use JetStream for persistence
    useJetstream: true
    # Stream name for traces
    streamName: XERV_TRACES

  # Raft configuration (when backend is "raft")
  raft:
    # Election timeout in milliseconds
    electionTimeoutMs: 1000
    # Heartbeat interval in milliseconds
    heartbeatIntervalMs: 100

# Server configuration
server:
  # HTTP API port
  apiPort: 8080
  # gRPC port (used for Raft peer communication)
  grpcPort: 5000

# Metrics configuration
metrics:
  enabled: true
  port: 9090
  path: /metrics
  serviceMonitor:
    enabled: false
    interval: 30s
    scrapeTimeout: 10s
    labels: {}

# Storage configuration
storage:
  # Storage class (leave empty for default)
  class: ""
  # Storage size
  size: 10Gi
  # Access mode
  accessMode: ReadWriteOnce

# Resource requests and limits
resources:
  requests:
    cpu: 100m
    memory: 256Mi
  limits:
    cpu: 1000m
    memory: 1Gi

# Service configuration
service:
  type: ClusterIP
  # Annotations for the service
  annotations: {}

# Ingress configuration
ingress:
  enabled: false
  className: ""
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # cert-manager.io/cluster-issuer: letsencrypt-prod
  hosts:
    - host: xerv.local
      paths:
        - path: /
          pathType: Prefix
  tls: []
    # - secretName: xerv-tls
    #   hosts:
    #     - xerv.local

# Pod security context
podSecurityContext:
  runAsNonRoot: true
  runAsUser: 1000
  fsGroup: 1000

# Container security context
securityContext:
  allowPrivilegeEscalation: false
  readOnlyRootFilesystem: true
  capabilities:
    drop:
      - ALL

# Service account
serviceAccount:
  create: true
  automount: true
  annotations: {}
  name: ""

# Node selector
nodeSelector: {}

# Tolerations
tolerations: []

# Affinity rules
affinity: {}

# Pod disruption budget
podDisruptionBudget:
  enabled: false
  minAvailable: 1
  # maxUnavailable: 1

# Liveness probe
livenessProbe:
  enabled: true
  initialDelaySeconds: 10
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

# Readiness probe
readinessProbe:
  enabled: true
  initialDelaySeconds: 5
  periodSeconds: 5
  timeoutSeconds: 3
  failureThreshold: 3

# Extra environment variables
extraEnv: []
  # - name: RUST_LOG
  #   value: xerv=debug

# Extra volume mounts
extraVolumeMounts: []
  # - name: pipelines
  #   mountPath: /pipelines
  #   readOnly: true

# Extra volumes
extraVolumes: []
  # - name: pipelines
  #   configMap:
  #     name: xerv-pipelines

# Horizontal Pod Autoscaler
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 10
  # Target CPU utilization percentage (optional)
  targetCPUUtilizationPercentage: 80
  # Target memory utilization percentage (optional)
  # targetMemoryUtilizationPercentage: 80

  # Custom XERV metrics for scaling
  metrics:
    # Scale based on pending traces in dispatch queue
    pendingTraces:
      enabled: false
      # Target average queue depth per pod
      targetValue: "100"
    # Scale based on active trace count
    activeTraces:
      enabled: false
      # Target average active traces per pod
      targetValue: "50"

  # Custom metrics (for advanced use cases)
  # customMetrics:
  #   - type: External
  #     external:
  #       metric:
  #         name: redis_queue_length
  #       target:
  #         type: Value
  #         value: "1000"

  # Scaling behavior configuration
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Percent
          value: 10
          periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
        - type: Percent
          value: 100
          periodSeconds: 15
        - type: Pods
          value: 4
          periodSeconds: 15
      selectPolicy: Max

# Network policy
networkPolicy:
  enabled: false
  ingress: []
  egress: []

# Service Mesh Integration
# Supports both Istio and Linkerd for advanced networking
serviceMesh:
  # Enable service mesh integration
  enabled: false
  # Provider: "istio" or "linkerd"
  provider: ""

  # Istio-specific configuration
  istio:
    # Enable Istio sidecar injection
    injection:
      enabled: true
    # DestinationRule configuration
    destinationRule:
      enabled: true
      # Traffic policy for load balancing
      trafficPolicy:
        connectionPool:
          http:
            # Upgrade HTTP/1.1 to HTTP/2
            h2UpgradePolicy: UPGRADE
            # Max requests per connection
            http1MaxPendingRequests: 100
            # Max concurrent HTTP/2 streams
            http2MaxRequests: 1000
          tcp:
            maxConnections: 100
        loadBalancer:
          simple: ROUND_ROBIN
          # Enable locality-aware load balancing
          localityLbSetting:
            enabled: true
        # Outlier detection for circuit breaking
        outlierDetection:
          consecutive5xxErrors: 5
          interval: 30s
          baseEjectionTime: 30s
          maxEjectionPercent: 50
      # TLS mode: DISABLE, SIMPLE, MUTUAL, ISTIO_MUTUAL
      tlsMode: ISTIO_MUTUAL
    # VirtualService configuration
    virtualService:
      enabled: true
      # Hosts to match (leave empty to use service name)
      hosts: []
      # Gateway to attach (optional, for external traffic)
      gateways: []
      # Timeout for requests
      timeout: 30s
      # Retry policy
      retries:
        attempts: 3
        perTryTimeout: 10s
        retryOn: "connect-failure,refused-stream,unavailable,cancelled,retriable-4xx,5xx"
    # PeerAuthentication for mTLS
    peerAuthentication:
      enabled: true
      # mTLS mode: DISABLE, PERMISSIVE, STRICT
      mtls: STRICT
    # AuthorizationPolicy (optional)
    authorizationPolicy:
      enabled: false
      # Rules to apply (empty means deny all if enabled)
      rules: []

  # Linkerd-specific configuration
  linkerd:
    # Enable Linkerd proxy injection
    injection:
      enabled: true
    # ServiceProfile for retries and timeouts
    serviceProfile:
      enabled: true
      # Route configurations
      routes:
        # Health endpoint - no retries
        - name: health
          condition:
            method: GET
            pathRegex: "/health"
          isRetryable: false
          timeout: 5s
        # Metrics endpoint - no retries
        - name: metrics
          condition:
            method: GET
            pathRegex: "/metrics"
          isRetryable: false
          timeout: 10s
        # API routes - retryable
        - name: api
          condition:
            method: "*"
            pathRegex: "/api/.*"
          isRetryable: true
          timeout: 30s
        # Default route
        - name: default
          condition:
            method: "*"
            pathRegex: ".*"
          isRetryable: true
          timeout: 30s
      # Default retry budget (percentage of requests that can be retried)
      retryBudget:
        retryRatio: 0.2
        minRetriesPerSecond: 10
        ttl: 10s
    # TrafficSplit for canary deployments
    trafficSplit:
      enabled: false
      # Canary configuration (only applies when enabled)
      canary:
        # Weight for stable version (0-1000, 1000 = 100%)
        stableWeight: 900
        # Weight for canary version (0-1000)
        canaryWeight: 100
        # Canary service name suffix
        canarySuffix: "-canary"

# Monitoring and Observability
monitoring:
  # Prometheus alerts configuration
  alerts:
    # Enable PrometheusRule creation
    enabled: false
    # Additional labels for PrometheusRule
    labels: {}
    # Annotations for PrometheusRule
    annotations: {}

    # Error rate thresholds
    # Warning threshold (5% = 0.05)
    errorRateThreshold: 0.05
    errorRateDuration: "5m"
    # Critical threshold (20% = 0.20)
    criticalErrorRateThreshold: 0.20
    criticalErrorRateDuration: "2m"

    # Queue depth thresholds
    queueDepthWarning: 100
    queueDepthDuration: "5m"
    queueDepthCritical: 500
    queueDepthCriticalDuration: "2m"

    # Raft leader alerts (only for raft backend)
    raftLeaderMissingDuration: "1m"
    raftLeaderFlappingThreshold: 3
    raftLeaderFlappingDuration: "5m"

    # Arena storage thresholds (in bytes)
    # Warning: 1GB = 1073741824
    arenaSizeWarning: 1073741824
    arenaSizeDuration: "5m"
    # Critical: 4GB = 4294967296
    arenaSizeCritical: 4294967296
    arenaSizeCriticalDuration: "2m"

    # Slow trace threshold (in seconds)
    slowTraceThreshold: 30
    slowTraceDuration: "10m"

    # No traces processed alert
    noTracesProcessedDuration: "5m"

    # Instance down alert
    instanceDownDuration: "1m"

  # Grafana dashboards
  dashboards:
    # Enable dashboard ConfigMap creation
    enabled: false
    # Labels for Grafana dashboard discovery
    labels:
      grafana_dashboard: "1"

  # Logging configuration
  logging:
    # Log format: json, pretty, compact
    format: json
    # Log level: trace, debug, info, warn, error
    level: info

  # Distributed tracing (OpenTelemetry)
  tracing:
    # Enable OpenTelemetry tracing
    enabled: false
    # OTLP endpoint (e.g., http://jaeger-collector:4317)
    endpoint: ""
    # Service name for traces
    serviceName: "xerv"
